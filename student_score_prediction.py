# -*- coding: utf-8 -*-
"""student_score_prediction_iosc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Uv5IM1ndzIhTpIMKFqMNUq4qAmmzC-V
"""

import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns

df=pd.read_csv("student_habits_performance.csv")

df.info()

df.drop(columns=['age','gender','parental_education_level','internet_quality','exercise_frequency','diet_quality'],inplace=True)

df

df["social_media_netflix_hrs"]=df.social_media_hours+df.netflix_hours

df=df.drop(columns=['social_media_hours','netflix_hours'])

df

categories=df.drop(columns=['student_id']).select_dtypes(object).columns.to_list()

for col in categories:
    df[col] = df[col].map({'Yes': 1, 'No': 0})

df

from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()

df_scaled=scaler.fit_transform(df.drop(columns=['student_id','exam_score']))

df_scaled=pd.DataFrame(df_scaled,columns=df.drop(columns=['student_id','exam_score']).columns)

df_scaled['exam_score']=df['exam_score']

df_scaled

correlation=df_scaled.corr()

sns.heatmap(correlation,annot=True)

"""## test,train,validation data"""

from sklearn.model_selection import train_test_split

train,test=train_test_split(df_scaled, random_state=42,train_size=0.8)

from sklearn.metrics import root_mean_squared_error as rmse

from xgboost import XGBRegressor

from sklearn.ensemble import RandomForestRegressor

from sklearn.model_selection import GridSearchCV

X_train=train.drop(columns=['exam_score'])
Y_train=train['exam_score']
X_test=test.drop(columns=['exam_score'])
Y_test=test['exam_score']

"""## XGBOOST hyperparameter tuning"""

# def XGB(X_train,Y_train):
#   cv=GridSearchCV(estimator=XGBRegressor(),param_grid={'n_estimators':[56,55,54,57],'max_depth':[2,3],'learning_rate':[0.16,0.2,0.25],'subsample':[0.66,0.69,0.7,0.72,0.73],'colsample_bytree':[0.45,0.44,0.46,0.47,0.48,0.4]},cv=3,n_jobs=-1)
#   cv.fit(X_train,Y_train)
#   param_xgboost=cv.best_params_
#   print(cv.best_params_)
#   return param_xgboost

# xgboost_params=XGB(X_train,Y_train)

def xgb_model(x_train,y_train,x_test,y_test):
  model=XGBRegressor(colsample_bytree= 0.4, learning_rate= 0.224, max_depth= 2, n_estimators= 53, subsample= 0.694, random_state=42)
  model.fit(x_train,y_train)
  prediction_test=model.predict(x_test)
  rmse_test=rmse(y_test,prediction_test)
  return rmse_test

rmse_xgbr=xgb_model(X_train,Y_train,X_test ,Y_test )

rmse_xgbr

"""## RANDOM FOREST  hyperparameter tuning"""

# def RandomForest(X_train,Y_train):
#   cv=GridSearchCV(estimator=RandomForestRegressor(),param_grid={'n_estimators':[27,28,29,30,35],'max_depth':[17,18,19,20],'min_samples_split':[2,3,4,5],'min_samples_leaf':[2,3,4]},cv=3,n_jobs=-1)
#   cv.fit(X_train,Y_train)
#   param_rforest=cv.best_params_
#   print(param_rforest)
#   return param_rforest

# rforest_params=RandomForest(X_train,Y_train)

def rgr_model(x_train,y_train,x_test,y_test):
  model=RandomForestRegressor(max_depth=19,min_samples_leaf=3,min_samples_split=4, n_estimators=29, random_state=42)
  model.fit(x_train,y_train)
  prediction_test=model.predict(x_test)
  rmse_test=rmse(y_test,prediction_test)
  return rmse_test

rmse_rfr=rgr_model(X_train,Y_train,X_test ,Y_test )

rmse_rfr

models=['XGBoost','RandomForest']

# model_selection=pd.DataFrame({'models':models,'params':[xgboost_params,rforest_params],'test_rmse':[rmse_xgbr,rmse_rfr]})

# model_selection

"""## bootstrapping"""

def rfr_model(x_test):
  model=RandomForestRegressor(max_depth=19,min_samples_leaf=3,min_samples_split=4, n_estimators=29, random_state=42,bootstrap=True)
  model.fit(X_train,Y_train)
  rfr_mean_prediction=model.predict(x_test)
  l=[]
  for i in model.estimators_:
    l.append(i.predict(x_test))
  trees=np.array(l)
  rfr_std_prediction=np.std(trees,axis=0)
  return rfr_mean_prediction,rfr_std_prediction

X_train_array=X_train.to_numpy()

Y_train_array=Y_train.to_numpy()

def xgb_model(x_test):
  model=XGBRegressor(colsample_bytree= 0.4, learning_rate= 0.224, max_depth= 2, n_estimators= 53, subsample= 0.694, random_state=42)

  n_bootstraps=40
  prediction=[]

  for i in range(n_bootstraps):
    index_array=np.random.choice(len(X_train_array),size=len(X_train_array))
    X_train_bootstrap=X_train_array[index_array]
    Y_train_bootstrap=Y_train_array[index_array]
    model.fit(X_train_bootstrap,Y_train_bootstrap)
    pred_bootstrap=model.predict(x_test)
    prediction.append(pred_bootstrap)
  xgb_mean_prediction=np.mean(prediction)
  xgb_std_prediction=np.std(prediction)
  return xgb_mean_prediction,xgb_std_prediction

def weights():
  weights=np.linspace(0,1,num=101)
  rmse_list=[]

  model_xgb=XGBRegressor(colsample_bytree= 0.4, learning_rate= 0.224, max_depth= 2, n_estimators= 53, subsample= 0.694, random_state=42)
  model_xgb.fit(X_train,Y_train)
  prediction_xgb=model_xgb.predict(X_test)

  model_rfr=RandomForestRegressor(max_depth=19,min_samples_leaf=3,min_samples_split=4, n_estimators=29, random_state=42,bootstrap=True)
  model_rfr.fit(X_train,Y_train)
  prediction_rfr=model_rfr.predict(X_test)

  for w_rfr in weights:
    ensemble_prediction=w_rfr*prediction_rfr+(1-w_rfr)*prediction_xgb
    rmse_calc=rmse(Y_test,ensemble_prediction)
    rmse_list.append([w_rfr,rmse_calc])

  rmse_min=min(rmse_list,key=lambda x:x[1])
  return rmse_min[0]

def ensemble_pred(rfr_mean_prediction,rfr_std_prediction,xgb_mean_prediction,xgb_std_prediction):
  w_rfr=weights()
  w_xgb=1-w_rfr
  ensemble_mean_prediction=w_rfr*rfr_mean_prediction+w_xgb*xgb_mean_prediction
  ensemble_std_prediction=w_rfr*rfr_std_prediction+w_xgb*xgb_std_prediction
  return ensemble_mean_prediction,ensemble_std_prediction

def bootstrapping(input_scaled):
  rfr_mean_prediction,rfr_std_prediction=rfr_model(input_scaled)
  xgb_mean_prediction,xgb_std_prediction=xgb_model(input_scaled)
  return ensemble_pred(rfr_mean_prediction,rfr_std_prediction,xgb_mean_prediction,xgb_std_prediction)

def predict(input_features):
  study_hours_per_day=input_features[0]
  part_time_job=input_features[1]
  attendance_percentage=input_features[2]
  sleep_hours=input_features[3]
  mental_health_rating=input_features[4]
  extracurricular_participation=input_features[5]
  social_media_netflix_hrs=input_features[6]

  input=pd.DataFrame({'study_hours_per_day':[study_hours_per_day],'part_time_job':[part_time_job],'attendance_percentage':[attendance_percentage],'sleep_hours':[sleep_hours],'mental_health_rating':[mental_health_rating],'extracurricular_participation':[extracurricular_participation],'social_media_netflix_hrs':[social_media_netflix_hrs]})
  input_scaled=scaler.transform(input)
  return bootstrapping(input_scaled)

def confidence_interval(input_features):
  mean_prediction,std_prediction=predict(input_features)
  mean = mean_prediction[0] if isinstance(mean_prediction, (np.ndarray, list)) else mean_prediction
  if mean<0:
    mean=0
  elif mean>100:
    mean=100
  else:
    mean = mean_prediction[0] if isinstance(mean_prediction, (np.ndarray, list)) else mean_prediction

  std = std_prediction[0] if isinstance(std_prediction, (np.ndarray, list)) else std_prediction
  upper=mean+1.96*std
  lower=mean-1.96*std
  width=upper-lower
  if width<10:
    return mean ,"HIGH cONFIDENCE"
  elif width<20:
    return mean ,"MODERATE CONFIDENCE"
  else:
    return mean ,"LOW CONFIDENCE"